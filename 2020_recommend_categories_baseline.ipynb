{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model for the problem of recommendations of categories to give a gift to a relative\n",
    "\n",
    "the model is analogous to the baseline for predicting purchases in merchant_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o08k0IFOCfM3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from bisect import bisect_left, bisect_right\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD0TKDyMCfM7"
   },
   "source": [
    "## Data\n",
    "\n",
    "Columns\n",
    "- `party_rk` – client unique identifier\n",
    "- `account_rk` – client account unique identifier\n",
    "- `financial_account_type_cd` – debit/credit card flag\n",
    "- `transaction_dttm` – operation datetime\n",
    "- `transaction_type_desc` – purchase/payment/...\n",
    "- `transaction_amt_rur` – transaction price\n",
    "- `merchant_type` - DUTY FREE STORES/FUEL DEALERS/RESTAURANTS/ etc\n",
    "- `merchant_group_rk` - McDonald's/Wildberries/ etc\n",
    "\n",
    "It's important that table is already sorted by `transaction_dttm` column!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "NxRbuZnfCfM8",
    "outputId": "1a3ba460-1737-45a5-bd51-3fa7abf8f176",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party_rk</th>\n",
       "      <th>account_rk</th>\n",
       "      <th>financial_account_type_cd</th>\n",
       "      <th>transaction_dttm</th>\n",
       "      <th>transaction_type_desc</th>\n",
       "      <th>transaction_amt_rur</th>\n",
       "      <th>merchant_rk</th>\n",
       "      <th>merchant_type</th>\n",
       "      <th>merchant_group_rk</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20337</td>\n",
       "      <td>19666</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>84.00</td>\n",
       "      <td>88676.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Сувениры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63404</td>\n",
       "      <td>72991</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>410.00</td>\n",
       "      <td>887248.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>Фаст Фуд</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24789</td>\n",
       "      <td>23517</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>701.44</td>\n",
       "      <td>830014.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Супермаркеты</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57970</td>\n",
       "      <td>64838</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>6203.70</td>\n",
       "      <td>363834.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>Дом/Ремонт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12232</td>\n",
       "      <td>11591</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>734.53</td>\n",
       "      <td>85919.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>878.0</td>\n",
       "      <td>Супермаркеты</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   party_rk  account_rk  financial_account_type_cd transaction_dttm  \\\n",
       "0     20337       19666                          1       2019-01-01   \n",
       "1     63404       72991                          1       2019-01-01   \n",
       "2     24789       23517                          2       2019-01-01   \n",
       "3     57970       64838                          2       2019-01-01   \n",
       "4     12232       11591                          2       2019-01-01   \n",
       "\n",
       "  transaction_type_desc  transaction_amt_rur  merchant_rk  merchant_type  \\\n",
       "0               Покупка                84.00      88676.0          348.0   \n",
       "1               Покупка               410.00     887248.0          330.0   \n",
       "2               Покупка               701.44     830014.0          291.0   \n",
       "3               Покупка              6203.70     363834.0          278.0   \n",
       "4               Покупка               734.53      85919.0          286.0   \n",
       "\n",
       "   merchant_group_rk      category  \n",
       "0                NaN      Сувениры  \n",
       "1              725.0      Фаст Фуд  \n",
       "2                NaN  Супермаркеты  \n",
       "3              454.0    Дом/Ремонт  \n",
       "4              878.0  Супермаркеты  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATADIR = \"hackathon_data\" # \"./data\"\n",
    "transactions_path = f\"{DATADIR}/avk_hackathon_data_transactions.csv\"\n",
    "pd.read_csv(f\"{DATADIR}/avk_hackathon_data_transactions.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VT0HZR2_CfNA"
   },
   "source": [
    "## Mappings\n",
    "~1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKHwvxB5CfNA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:43<00:00, 17.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare & save mappings\n",
    "mappings = defaultdict(dict)\n",
    "unk_token = \"<UNK>\"\n",
    "\n",
    "\n",
    "def create_mapping(values):\n",
    "    mapping = {unk_token: 0}\n",
    "    for v in values:\n",
    "        if not pd.isna(v):\n",
    "            mapping[str(v)] = len(mapping)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "for col in tqdm(\n",
    "    [\n",
    "        \"transaction_type_desc\",\n",
    "        \"merchant_rk\",\n",
    "        \"merchant_type\",\n",
    "        \"merchant_group_rk\",\n",
    "        \"category\",\n",
    "        \"financial_account_type_cd\",\n",
    "    ]\n",
    "):\n",
    "\n",
    "    col_values = (\n",
    "        pd.read_csv(transactions_path, usecols=[col])[col]\n",
    "        .fillna(unk_token)\n",
    "        .astype(str)\n",
    "    )\n",
    "    mappings[col] = create_mapping(col_values.unique())\n",
    "    del col_values\n",
    "\n",
    "\n",
    "#with open(f\"{DATADIR}/mappings.json\", \"w\") as f:\n",
    "#    json.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_2kzArrCfND"
   },
   "outputs": [],
   "source": [
    "# load mappings\n",
    "#with open(f\"{DATADIR}/mappings.json\", 'r') as f:\n",
    "#    mappings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "x3iE5io4ODte",
    "outputId": "a09c2d7c-8a32-4c42-af0a-1863a693e24b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 7,\n",
       " 'Сувениры': 1,\n",
       " 'Фаст Фуд': 2,\n",
       " 'Супермаркеты': 3,\n",
       " 'Дом/Ремонт': 4,\n",
       " 'Сервисные услуги': 5,\n",
       " 'Красота': 6,\n",
       " 'Разные товары': 7,\n",
       " 'Транспорт': 8,\n",
       " 'Медицинские услуги': 9,\n",
       " 'Топливо': 10,\n",
       " 'Одежда/Обувь': 11,\n",
       " 'Наличные': 12,\n",
       " 'Связь/Телеком': 13,\n",
       " 'Частные услуги': 14,\n",
       " 'Финансовые услуги': 15,\n",
       " 'Рестораны': 16,\n",
       " 'Развлечения': 17,\n",
       " 'НКО': 18,\n",
       " 'Книги': 19,\n",
       " 'Кино': 20,\n",
       " 'Автоуслуги': 21,\n",
       " 'Музыка': 22,\n",
       " 'Отели': 23,\n",
       " 'Аптеки': 24,\n",
       " 'Цветы': 25,\n",
       " 'Ж/д билеты': 26,\n",
       " 'Авиабилеты': 27,\n",
       " 'Спорттовары': 28,\n",
       " 'Госсборы': 29,\n",
       " 'Аренда авто': 30,\n",
       " 'Животные': 31,\n",
       " 'Duty Free': 32,\n",
       " 'Турагентства': 33,\n",
       " 'Образование': 34,\n",
       " 'Искусство': 35,\n",
       " 'Фото/Видео': 36}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ezUae-0CfNG"
   },
   "outputs": [],
   "source": [
    "# Prepare & save client data\n",
    "party2dates = defaultdict(list)  # for each party save a series of the transaction dates\n",
    "party2sum = defaultdict(list)  # for each party save a series of the transaction costs \n",
    "party2merchant_type = defaultdict(list)  # for each party save a series of the transaction_type \n",
    "party2trans_type = defaultdict(list)  # for each party save a series of the transaction merchant_type\n",
    "\n",
    "usecols = [\n",
    "    \"party_rk\",\n",
    "    \"transaction_dttm\",\n",
    "    \"transaction_amt_rur\",\n",
    "    \"merchant_type\",\n",
    "    \"transaction_type_desc\",\n",
    "]\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(transactions_path, usecols=usecols, chunksize=100_000)\n",
    "):\n",
    "\n",
    "    chunk[\"merchant_type\"] = (\n",
    "        chunk[\"merchant_type\"].fillna(unk_token).astype(str)\n",
    "    )\n",
    "    chunk[\"transaction_type_desc\"] = (\n",
    "        chunk[\"transaction_type_desc\"].fillna(unk_token).astype(str)\n",
    "    )\n",
    "    chunk[\"transaction_amt_rur\"] = chunk[\"transaction_amt_rur\"].fillna(0)\n",
    "\n",
    "    for i, row in chunk.iterrows():\n",
    "        party2dates[row.party_rk].append(row.transaction_dttm)\n",
    "        party2sum[row.party_rk].append(row.transaction_amt_rur)\n",
    "        party2merchant_type[row.party_rk].append(\n",
    "            mappings[\"merchant_type\"][row.merchant_type]\n",
    "        )\n",
    "        party2trans_type[row.party_rk].append(\n",
    "            mappings[\"transaction_type_desc\"][row.transaction_type_desc]\n",
    "        )\n",
    "\n",
    "    del chunk\n",
    "\n",
    "pickle.dump(party2dates, open(f\"{DATADIR}/party2dates.pkl\", \"wb\"))\n",
    "pickle.dump(party2sum, open(f\"{DATADIR}/party2sum.pkl\", \"wb\"))\n",
    "pickle.dump(party2merchant_type, open(f\"{DATADIR}/party2merchant_type.pkl\", \"wb\"))\n",
    "pickle.dump(party2trans_type, open(f\"{DATADIR}/party2trans_type.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S9TuIiAu-v55",
    "outputId": "26b5e981-2c2f-44e9-f15d-25858e7baa95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [32:00, 16.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare & save client data\n",
    "party2category = defaultdict(list)  # for each party save a series of the transaction categories\n",
    "\n",
    "usecols = [\n",
    "           \"party_rk\",\n",
    "          \"category\"\n",
    "]\n",
    "\n",
    "for chunk in tqdm(\n",
    "    pd.read_csv(transactions_path, usecols=usecols, chunksize=100_000)\n",
    "):\n",
    "    chunk[\"category\"] = (\n",
    "        chunk[\"category\"].fillna(unk_token).astype(str)\n",
    "    )\n",
    "    for i, row in chunk.iterrows():\n",
    "        party2category[row.party_rk].append(\n",
    "            mappings[\"category\"][row.category]\n",
    "        )\n",
    "    del chunk\n",
    "\n",
    "pickle.dump(party2category, open(f\"{DATADIR}/party2category.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQfeDGZZCfNJ"
   },
   "outputs": [],
   "source": [
    "# load client data\n",
    "party2dates = pickle.load(open(f\"{DATADIR}/party2dates.pkl\", 'rb'))\n",
    "party2sum = pickle.load(open(f\"{DATADIR}/party2sum.pkl\", 'rb'))\n",
    "party2merchant_type = pickle.load(open(f\"{DATADIR}/party2merchant_type.pkl\", 'rb'))\n",
    "party2trans_type = pickle.load(open(f\"{DATADIR}/party2trans_type.pkl\", 'rb'))\n",
    "party2category = pickle.load(open(f\"{DATADIR}/party2category.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtLQTSfDCfNL"
   },
   "source": [
    "## PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uf2Pmq0FCfNM",
    "outputId": "a016f11e-3ef4-471d-da5b-da6e440f49f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 40000 Val: 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_party, valid_party = train_test_split(\n",
    "    pd.read_csv(transactions_path, usecols=['party_rk']).party_rk.unique(), \n",
    "    train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_party)} Val: {len(valid_party)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGZuEgEoCfNO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_period_len = 60  # -- days\n",
    "train_predict_dates = (\n",
    "    pd.date_range(\"2019-03-01\", \"2019-10-31\", freq=\"MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "valid_predict_dates = (\n",
    "    pd.date_range(\"2019-11-01\", \"2019-12-31\", freq=\"MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "submission_predict_dates = (\n",
    "    pd.date_range(\"2020-01-01\", \"2020-02-28\", freq=\"2MS\")\n",
    "    .strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6rAD6SdCfNR"
   },
   "outputs": [],
   "source": [
    "def prepare_data(party_list, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    This function define the pipeline of the creation of train and valid samples.\n",
    "    We consider each client from party_list. For each client take each \n",
    "    predict_period_start from predict_dates list. All client transaction before\n",
    "    this date is our features. Next, we look at the customer's transactions in \n",
    "    the next two months. This transactions should be predicted. It will form\n",
    "    our labels vector.\n",
    "    \"\"\"\n",
    "\n",
    "    data_sum = []\n",
    "    data_trans_type = []\n",
    "    data_category = []\n",
    "    data_labels = []\n",
    "\n",
    "    for party_rk in tqdm(party_list):\n",
    "        date_series = party2dates[party_rk]\n",
    "        sum_series = party2sum[party_rk]\n",
    "        merch_type_series = party2merchant_type[party_rk]\n",
    "        trans_type_series = party2trans_type[party_rk]\n",
    "        category_series = party2category[party_rk]\n",
    "\n",
    "        if mode == \"train\":\n",
    "            predict_dates = train_predict_dates\n",
    "        elif mode == \"valid\":\n",
    "            predict_dates = valid_predict_dates\n",
    "        elif mode == \"submission\":\n",
    "            predict_dates = submission_predict_dates\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode\")\n",
    "\n",
    "        for predict_period_start in predict_dates:\n",
    "\n",
    "            predict_period_end = datetime.strftime(\n",
    "                datetime.strptime(predict_period_start, \"%Y-%m-%d\")\n",
    "                + timedelta(days=predict_period_len),\n",
    "                \"%Y-%m-%d\",\n",
    "            )\n",
    "\n",
    "            l, r = (\n",
    "                bisect_left(date_series, predict_period_start),\n",
    "                bisect_right(date_series, predict_period_end),\n",
    "            )\n",
    "\n",
    "            history_category = category_series[:l]\n",
    "            history_sum = sum_series[:l]\n",
    "            history_trans_type = trans_type_series[:l]\n",
    "            predict_category = category_series[l:r]\n",
    "\n",
    "            if predict_category and l or mode not in (\"train\", \"valid\"):\n",
    "                data_sum.append(history_sum)\n",
    "                data_trans_type.append(history_trans_type)\n",
    "                data_category.append(history_category)\n",
    "                data_labels.append(predict_category)\n",
    "\n",
    "    return data_sum, data_trans_type, data_category, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zZzpIPNNCfNT",
    "outputId": "895566b4-45a4-4c98-8e90-bf0e2c45701a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:15<00:00, 2537.49it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 11163.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sum, train_trans_type, train_category, train_labels = prepare_data(\n",
    "    train_party, mode=\"train\"\n",
    ")\n",
    "valid_sum, valid_trans_type, valid_category, valid_labels = prepare_data(\n",
    "    valid_party, mode=\"valid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-xdScilCfNW"
   },
   "source": [
    "## PyTorch loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gUqipggCfNX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex8SKiPqCfNZ"
   },
   "outputs": [],
   "source": [
    "MERCH_TYPE_NCLASSES = len(mappings['merchant_type'])\n",
    "CATEGORY_NCLASSES = len(mappings['category'])\n",
    "TRANS_TYPE_NCLASSES = len(mappings['transaction_type_desc'])\n",
    "PADDING_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjllhPWGCfNb"
   },
   "outputs": [],
   "source": [
    "class RSDataset(Dataset):\n",
    "    def __init__(self, data_sum, data_trans_type, data_category, labels):\n",
    "        super(RSDataset, self).__init__()\n",
    "        self.data_sum = data_sum\n",
    "        self.data_trans_type = data_trans_type\n",
    "        self.data_category = data_category\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_sum)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        targets = np.zeros((CATEGORY_NCLASSES - 1,), dtype=np.float32)\n",
    "        for m in self.labels[idx]:\n",
    "            if m:  # skip UNK, UNK-token should not be predicted\n",
    "                targets[m - 1] = 1.0\n",
    "\n",
    "        item = {\n",
    "            \"features\": {},\n",
    "            \"targets\": targets,\n",
    "        }\n",
    "\n",
    "        sum_feature = np.array(self.data_sum[idx][-PADDING_LEN:])\n",
    "        sum_feature = np.vectorize(lambda s: np.log(1 + s))(sum_feature)\n",
    "        if sum_feature.shape[0] < PADDING_LEN:\n",
    "            pad = np.zeros(\n",
    "                (PADDING_LEN - sum_feature.shape[0],), dtype=np.float32\n",
    "            )\n",
    "            sum_feature = np.hstack((sum_feature, pad))\n",
    "        item[\"features\"][\"sum\"] = torch.from_numpy(sum_feature).float()\n",
    "\n",
    "        for feature_name, feature_values in zip(\n",
    "            [\"trans_type\", \"category\"],\n",
    "            [self.data_trans_type[idx], self.data_category[idx]],\n",
    "        ):\n",
    "\n",
    "            feature_values = np.array(feature_values[-PADDING_LEN:])\n",
    "            mask = np.ones(feature_values.shape[0], dtype=np.float32)\n",
    "            if feature_values.shape[0] < PADDING_LEN:\n",
    "                feature_values = np.append(\n",
    "                    feature_values,\n",
    "                    np.zeros(\n",
    "                        PADDING_LEN - feature_values.shape[0], dtype=np.int64\n",
    "                    ),\n",
    "                )\n",
    "                mask = np.append(\n",
    "                    mask,\n",
    "                    np.zeros(PADDING_LEN - mask.shape[0], dtype=np.float32),\n",
    "                )\n",
    "            item[\"features\"][feature_name] = torch.from_numpy(feature_values).long()\n",
    "            item[\"features\"][f\"{feature_name}_mask\"] = torch.from_numpy(mask).float()\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3-aV7lJCfNd"
   },
   "outputs": [],
   "source": [
    "train_dataset = RSDataset(\n",
    "    train_sum, train_trans_type, train_category, train_labels\n",
    ")\n",
    "valid_dataset = RSDataset(\n",
    "    valid_sum, valid_trans_type, valid_category, valid_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8jnB23BCfNg"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=64, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGRUJWR3CfNi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3803/3803 [22:10<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for i in tqdm(range(len(train_loader))):\n",
    "    batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J67Ms2_jCfNk"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kWquDGFCfNl"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-PAEoqoCfNo"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'category_emb_dim': 16,\n",
    "    'trans_type_embedding': 3,\n",
    "    'transformer_nhead': 2,\n",
    "    'transformer_dim_feedforward': 256,\n",
    "    'transformer_dropout': 0.1,\n",
    "    'dense_unit': 256,\n",
    "    'num_layers': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NeAfj7h_CfNq",
    "outputId": "42242254-9ce2-47a7-e3bf-9d1556587026"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORY_NCLASSES, TRANS_TYPE_NCLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDhr4svJCfNs"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.category_embedding = nn.Embedding(\n",
    "            MERCH_TYPE_NCLASSES, params[\"category_emb_dim\"]\n",
    "        )\n",
    "        self.trans_type_embedding = nn.Embedding(\n",
    "            TRANS_TYPE_NCLASSES, params[\"trans_type_embedding\"]\n",
    "        )\n",
    "\n",
    "        embedding_size = (\n",
    "            params[\"category_emb_dim\"]\n",
    "            + params[\"trans_type_embedding\"]\n",
    "            + 1\n",
    "        )\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for i in range(params[\"num_layers\"]):\n",
    "            transformer_block = nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=params[\"transformer_nhead\"],\n",
    "                dim_feedforward=params[\"transformer_dim_feedforward\"],\n",
    "                dropout=params[\"transformer_dropout\"],\n",
    "            )\n",
    "            transformer_blocks.append(\n",
    "                (f\"transformer_block_{i}\", transformer_block)\n",
    "            )\n",
    "\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            OrderedDict(transformer_blocks)\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embedding_size, out_features=params[\"dense_unit\"]\n",
    "        )\n",
    "        self.scorer = nn.Linear(\n",
    "            in_features=params[\"dense_unit\"],\n",
    "            out_features=CATEGORY_NCLASSES - 1,\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        category_emb = self.category_embedding(features[\"category\"])\n",
    "        trans_type_emb = self.trans_type_embedding(features[\"trans_type\"])\n",
    "\n",
    "        category_emb = category_emb * features[\"category_mask\"].unsqueeze(-1)\n",
    "        trans_type_emb = trans_type_emb * features[\"trans_type_mask\"].unsqueeze(-1)\n",
    "\n",
    "        embeddings = torch.cat(\n",
    "            (category_emb, trans_type_emb, features[\"sum\"].unsqueeze(-1)),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "        pooling = torch.mean(transformer_output, dim=1)\n",
    "        linear = torch.tanh(self.linear(pooling))\n",
    "        merch_logits = self.scorer(linear)\n",
    "\n",
    "        return merch_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NszoLAhbCfNu"
   },
   "source": [
    "### One-batch-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PVdmq86KCfNu",
    "outputId": "54f31c14-695b-4521-ea1e-1093074f6f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6881, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "batch = next(iter(train_loader))\n",
    "output = model(batch['features'])\n",
    "loss = criterion(output, batch['targets'])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZz71tEDCfNw"
   },
   "source": [
    "## Train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
    "\n",
    "[A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcF4frWZCfNx"
   },
   "outputs": [],
   "source": [
    "from catalyst import dl, utils\n",
    "from catalyst.utils import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6EBSFvEwpQl"
   },
   "source": [
    "## Custom metrics for this hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OhGkjT0CfNz"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catalyst.utils.metrics.functional import preprocess_multi_label_metrics\n",
    "from catalyst.utils.torch import get_activation_fn\n",
    "\n",
    "\n",
    "def multi_label_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    threshold: Union[float, torch.Tensor],\n",
    "    activation: Optional[str] = None,\n",
    "    eps: float = 1e-7,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes multi-label precision for the specified activation and threshold.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "            indicates the probability of the example belonging to each of\n",
    "            the K classes, according to the model.\n",
    "        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "            classes are associated with the N-th input\n",
    "            (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "            associated with classes 2 and 4)\n",
    "        threshold (float): threshold for for model output\n",
    "        activation (str): activation to use for model output\n",
    "        eps (float): epsilon to avoid zero division\n",
    "    \n",
    "    Extended version of \n",
    "        https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/metrics/accuracy.py#L58\n",
    "\n",
    "    Returns:\n",
    "        computed multi-label metrics\n",
    "    \"\"\"\n",
    "    outputs, targets, _ = preprocess_multi_label_metrics(\n",
    "        outputs=outputs, targets=targets\n",
    "    )\n",
    "    activation_fn = get_activation_fn(activation)\n",
    "    outputs = activation_fn(outputs)\n",
    "\n",
    "    outputs = (outputs > threshold).long()\n",
    "\n",
    "    accuracy = (targets.long() == outputs.long()).sum().float() / np.prod(\n",
    "        targets.shape\n",
    "    )\n",
    "\n",
    "    intersection = (outputs.long() * targets.long()).sum(axis=1).float()\n",
    "    num_predicted = outputs.long().sum(axis=1).float()\n",
    "    num_relevant = targets.long().sum(axis=1).float()\n",
    "    union = num_predicted + num_relevant\n",
    "\n",
    "    # Precision = ({predicted items} && {relevant items}) / {predicted items}\n",
    "    precision = intersection / (num_predicted + eps * (num_predicted == 0))\n",
    "    # Recall = ({predicted items} && {relevant items}) / {relevant items}\n",
    "    recall = intersection / (num_relevant + eps * (num_relevant == 0))\n",
    "    # IoU = ({predicted items} && {relevant items}) / ({predicted items} || {relevant items})\n",
    "    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n",
    "\n",
    "    return accuracy, precision.mean(), recall.mean(), iou.mean()\n",
    "\n",
    "\n",
    "def precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "       actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "       predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       k (int): parameter k of precison@k\n",
    "\n",
    "    Returns:\n",
    "       Computed value of precision@k for given sample\n",
    "    \"\"\"\n",
    "    p_at_k = 0.0\n",
    "    for item in predicted[:k]:\n",
    "        if actual[item]:\n",
    "            p_at_k += 1\n",
    "    p_at_k /= k\n",
    "\n",
    "    return p_at_k\n",
    "\n",
    "\n",
    "def average_precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes average precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "      actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "      predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "          classes are associated with the N-th input\n",
    "          (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "          associated with classes 2 and 4)\n",
    "      k (int): parameter k of AP@k\n",
    "\n",
    "    Returns:\n",
    "        Computed value of AP@k for given sample\n",
    "    \"\"\"\n",
    "    ap_at_k = 0.0\n",
    "    for idx, item in enumerate(predicted[:k]):\n",
    "        if actual[item]:\n",
    "            ap_at_k += precision_at_k(actual, predicted, k=idx + 1)\n",
    "    ap_at_k /= min(k, actual.sum().cpu().numpy())\n",
    "    \n",
    "\n",
    "    return ap_at_k\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(\n",
    "    output: torch.Tensor, target: torch.Tensor, top_k: Tuple[int, ...] = (1,)\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes mean_average_precision_at_k at set of cutoff parameters K\n",
    "\n",
    "    Args:\n",
    "       outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "           indicates the probability of the example belonging to each of\n",
    "           the K classes, according to the model.\n",
    "       targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       top_k (tuple): list of parameters k at which map@k will be computed\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       List of computed values of map@k at each cutoff k from topk\n",
    "    \"\"\"\n",
    "    max_k = max(top_k)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, top_indices = output.topk(k=max_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "    result = []\n",
    "    for k in top_k:  # loop over k\n",
    "        map_at_k = 0.0\n",
    "        for actual_target, predicted_items in zip(\n",
    "            target, top_indices\n",
    "        ):  # loop over samples\n",
    "            map_at_k += average_precision_at_k(\n",
    "                actual_target, predicted_items, k\n",
    "            )\n",
    "        map_at_k = map_at_k / batch_size\n",
    "        result.append(map_at_k)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GygG0d1WCfN2"
   },
   "outputs": [],
   "source": [
    "# What is Runner?\n",
    "# https://catalyst-team.github.io/catalyst/api/core.html#runner\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        features, targets = batch[\"features\"], batch[\"targets\"]\n",
    "        logits = self.model(features)\n",
    "        scores = torch.sigmoid(logits)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "        accuracy, precision, recall, iou = multi_label_metrics(\n",
    "            logits, targets, threshold=0.5, activation=\"Sigmoid\"\n",
    "        )\n",
    "        map05, map10, map20, map30 = mean_average_precision_at_k(\n",
    "            scores, targets, top_k=(5, 10, 20, 30)\n",
    "        )\n",
    "        batch_metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"iou\": iou,\n",
    "            \"map05\": map05,\n",
    "            \"map10\": map10,\n",
    "            \"map20\": map20,\n",
    "            \"map30\": map30,\n",
    "        }\n",
    "        \n",
    "        self.input = {\"features\": features, \"targets\": targets}\n",
    "        self.output = {\"logits\": logits, \"scores\": scores}\n",
    "        self.batch_metrics.update(batch_metrics)\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def predict_batch(self, batch):\n",
    "        # model inference step\n",
    "        batch = utils.maybe_recursive_call(batch, \"to\", device=self.device)\n",
    "        logits = self.model(batch[\"features\"])\n",
    "        scores = torch.sigmoid(logits)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noTAPVPSCfN5"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mhxJPaHTCfN6",
    "outputId": "349441fd-b42b-437c-d866-7222a77499ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 * Epoch (train): 100% 3803/3803 [42:50<00:00,  1.48it/s, accuracy=0.863, iou=0.538, loss=0.323, map05=0.897, map10=0.822, map20=0.826, map30=0.852, precision=0.856, recall=0.617]\n",
      "1/10 * Epoch (valid): 100% 271/271 [02:44<00:00,  1.65it/s, accuracy=0.899, iou=0.499, loss=0.264, map05=0.777, map10=0.772, map20=0.793, map30=0.811, precision=0.707, recall=0.694]\n",
      "[2020-09-19 18:53:50,497] \n",
      "1/10 * Epoch 1 (train): accuracy=0.8656 | iou=0.4411 | loss=0.3248 | map05=0.7709 | map10=0.7344 | map20=0.7592 | map30=0.7828 | precision=0.7550 | recall=0.5782\n",
      "1/10 * Epoch 1 (valid): accuracy=0.8713 | iou=0.4672 | loss=0.3165 | map05=0.7892 | map10=0.7506 | map20=0.7744 | map30=0.7947 | precision=0.6947 | recall=0.6681\n",
      "2/10 * Epoch (train): 100% 3803/3803 [42:27<00:00,  1.49it/s, accuracy=0.887, iou=0.510, loss=0.288, map05=0.805, map10=0.780, map20=0.798, map30=0.823, precision=0.808, recall=0.638]\n",
      "2/10 * Epoch (valid): 100% 271/271 [02:44<00:00,  1.65it/s, accuracy=0.895, iou=0.476, loss=0.257, map05=0.815, map10=0.778, map20=0.807, map30=0.822, precision=0.690, recall=0.681]\n",
      "[2020-09-19 19:39:01,907] \n",
      "2/10 * Epoch 2 (train): accuracy=0.8724 | iou=0.4653 | loss=0.3112 | map05=0.8030 | map10=0.7628 | map20=0.7844 | map30=0.8068 | precision=0.7787 | recall=0.5956\n",
      "2/10 * Epoch 2 (valid): accuracy=0.8734 | iou=0.4708 | loss=0.3116 | map05=0.7989 | map10=0.7592 | map20=0.7823 | map30=0.8025 | precision=0.7108 | recall=0.6585\n",
      "3/10 * Epoch (train): 100% 3803/3803 [41:49<00:00,  1.52it/s, accuracy=0.909, iou=0.533, loss=0.253, map05=0.853, map10=0.816, map20=0.858, map30=0.868, precision=0.825, recall=0.683]\n",
      "3/10 * Epoch (valid): 100% 271/271 [02:41<00:00,  1.68it/s, accuracy=0.907, iou=0.524, loss=0.256, map05=0.798, map10=0.788, map20=0.802, map30=0.821, precision=0.704, recall=0.729]\n",
      "[2020-09-19 20:23:33,363] \n",
      "3/10 * Epoch 3 (train): accuracy=0.8744 | iou=0.4736 | loss=0.3064 | map05=0.8108 | map10=0.7710 | map20=0.7917 | map30=0.8135 | precision=0.7834 | recall=0.6026\n",
      "3/10 * Epoch 3 (valid): accuracy=0.8748 | iou=0.4788 | loss=0.3072 | map05=0.8092 | map10=0.7704 | map20=0.7926 | map30=0.8119 | precision=0.6983 | recall=0.6818\n",
      "4/10 * Epoch (train): 100% 3803/3803 [38:58<00:00,  1.63it/s, accuracy=0.855, iou=0.414, loss=0.337, map05=0.789, map10=0.755, map20=0.766, map30=0.791, precision=0.725, recall=0.594]\n",
      "4/10 * Epoch (valid): 100% 271/271 [02:10<00:00,  2.08it/s, accuracy=0.900, iou=0.499, loss=0.251, map05=0.825, map10=0.809, map20=0.827, map30=0.839, precision=0.702, recall=0.694]\n",
      "[2020-09-19 21:04:42,503] \n",
      "4/10 * Epoch 4 (train): accuracy=0.8759 | iou=0.4801 | loss=0.3028 | map05=0.8169 | map10=0.7780 | map20=0.7980 | map30=0.8194 | precision=0.7860 | recall=0.6093\n",
      "4/10 * Epoch 4 (valid): accuracy=0.8775 | iou=0.4831 | loss=0.2990 | map05=0.8172 | map10=0.7798 | map20=0.8014 | map30=0.8203 | precision=0.7170 | recall=0.6712\n",
      "5/10 * Epoch (train): 100% 3803/3803 [34:23<00:00,  1.84it/s, accuracy=0.889, iou=0.570, loss=0.280, map05=0.859, map10=0.792, map20=0.802, map30=0.829, precision=0.793, recall=0.676]\n",
      "5/10 * Epoch (valid): 100% 271/271 [02:17<00:00,  1.98it/s, accuracy=0.892, iou=0.486, loss=0.262, map05=0.816, map10=0.804, map20=0.817, map30=0.832, precision=0.673, recall=0.697]\n",
      "[2020-09-19 21:41:22,655] \n",
      "5/10 * Epoch 5 (train): accuracy=0.8770 | iou=0.4844 | loss=0.3004 | map05=0.8212 | map10=0.7824 | map20=0.8020 | map30=0.8231 | precision=0.7871 | recall=0.6141\n",
      "5/10 * Epoch 5 (valid): accuracy=0.8752 | iou=0.4857 | loss=0.3036 | map05=0.8164 | map10=0.7796 | map20=0.8005 | map30=0.8196 | precision=0.7000 | recall=0.6907\n",
      "6/10 * Epoch (train): 100% 3803/3803 [35:20<00:00,  1.79it/s, accuracy=0.877, iou=0.559, loss=0.298, map05=0.909, map10=0.798, map20=0.848, map30=0.867, precision=0.919, recall=0.588]\n",
      "6/10 * Epoch (valid): 100% 271/271 [02:11<00:00,  2.05it/s, accuracy=0.895, iou=0.496, loss=0.263, map05=0.810, map10=0.801, map20=0.826, map30=0.836, precision=0.664, recall=0.715]\n",
      "[2020-09-19 22:18:55,306] \n",
      "6/10 * Epoch 6 (train): accuracy=0.8779 | iou=0.4884 | loss=0.2983 | map05=0.8244 | map10=0.7857 | map20=0.8050 | map30=0.8258 | precision=0.7883 | recall=0.6183\n",
      "6/10 * Epoch 6 (valid): accuracy=0.8756 | iou=0.4880 | loss=0.3034 | map05=0.8209 | map10=0.7835 | map20=0.8042 | map30=0.8229 | precision=0.6950 | recall=0.7012\n",
      "7/10 * Epoch (train): 100% 3803/3803 [34:07<00:00,  1.86it/s, accuracy=0.895, iou=0.551, loss=0.267, map05=0.832, map10=0.754, map20=0.802, map30=0.822, precision=0.810, recall=0.683]\n",
      "7/10 * Epoch (valid): 100% 271/271 [02:18<00:00,  1.96it/s, accuracy=0.897, iou=0.507, loss=0.258, map05=0.842, map10=0.821, map20=0.839, map30=0.855, precision=0.672, recall=0.726]\n",
      "[2020-09-19 22:55:21,035] \n",
      "7/10 * Epoch 7 (train): accuracy=0.8786 | iou=0.4912 | loss=0.2969 | map05=0.8265 | map10=0.7880 | map20=0.8071 | map30=0.8276 | precision=0.7882 | recall=0.6219\n",
      "7/10 * Epoch 7 (valid): accuracy=0.8772 | iou=0.4894 | loss=0.2984 | map05=0.8234 | map10=0.7872 | map20=0.8077 | map30=0.8262 | precision=0.6981 | recall=0.7004\n",
      "8/10 * Epoch (train): 100% 3803/3803 [35:51<00:00,  1.77it/s, accuracy=0.872, iou=0.475, loss=0.322, map05=0.841, map10=0.739, map20=0.805, map30=0.809, precision=0.811, recall=0.583]\n",
      "8/10 * Epoch (valid): 100% 271/271 [02:14<00:00,  2.01it/s, accuracy=0.901, iou=0.515, loss=0.258, map05=0.821, map10=0.813, map20=0.831, map30=0.844, precision=0.690, recall=0.730]\n",
      "[2020-09-19 23:33:27,907] \n",
      "8/10 * Epoch 8 (train): accuracy=0.8791 | iou=0.4936 | loss=0.2957 | map05=0.8276 | map10=0.7897 | map20=0.8086 | map30=0.8290 | precision=0.7879 | recall=0.6248\n",
      "8/10 * Epoch 8 (valid): accuracy=0.8791 | iou=0.4914 | loss=0.2954 | map05=0.8246 | map10=0.7880 | map20=0.8091 | map30=0.8269 | precision=0.7087 | recall=0.6926\n",
      "9/10 * Epoch (train): 100% 3803/3803 [35:57<00:00,  1.76it/s, accuracy=0.894, iou=0.494, loss=0.299, map05=0.810, map10=0.773, map20=0.805, map30=0.824, precision=0.786, recall=0.648]\n",
      "9/10 * Epoch (valid): 100% 271/271 [02:19<00:00,  1.94it/s, accuracy=0.896, iou=0.524, loss=0.275, map05=0.824, map10=0.817, map20=0.832, map30=0.846, precision=0.663, recall=0.770]\n",
      "[2020-09-20 00:11:44,821] \n",
      "9/10 * Epoch 9 (train): accuracy=0.8794 | iou=0.4951 | loss=0.2946 | map05=0.8290 | map10=0.7911 | map20=0.8101 | map30=0.8304 | precision=0.7876 | recall=0.6271\n",
      "9/10 * Epoch 9 (valid): accuracy=0.8747 | iou=0.4889 | loss=0.3049 | map05=0.8230 | map10=0.7852 | map20=0.8059 | map30=0.8247 | precision=0.6792 | recall=0.7165\n",
      "10/10 * Epoch (train): 100% 3803/3803 [35:31<00:00,  1.78it/s, accuracy=0.895, iou=0.523, loss=0.253, map05=0.834, map10=0.803, map20=0.844, map30=0.850, precision=0.676, recall=0.719]\n",
      "10/10 * Epoch (valid): 100% 271/271 [02:15<00:00,  2.00it/s, accuracy=0.893, iou=0.522, loss=0.279, map05=0.818, map10=0.803, map20=0.820, map30=0.834, precision=0.640, recall=0.792]\n",
      "[2020-09-20 00:49:31,565] \n",
      "10/10 * Epoch 10 (train): accuracy=0.8798 | iou=0.4967 | loss=0.2938 | map05=0.8300 | map10=0.7921 | map20=0.8111 | map30=0.8312 | precision=0.7872 | recall=0.6292\n",
      "10/10 * Epoch 10 (valid): accuracy=0.8732 | iou=0.4892 | loss=0.3084 | map05=0.8211 | map10=0.7844 | map20=0.8062 | map30=0.8242 | precision=0.6668 | recall=0.7297\n",
      "Top best models:\n",
      "logs_cats/checkpoints/train.8.pth\t0.4914\n",
      "=> Loading checkpoint logs_cats/checkpoints/best_full.pth\n",
      "loaded state checkpoint logs_cats/checkpoints/best_full.pth (global epoch 8, epoch 8, stage train)\n"
     ]
    }
   ],
   "source": [
    "# For other minimal examples, please follow the link below\n",
    "# https://github.com/catalyst-team/catalyst#minimal-examples\n",
    "runner = CustomRunner()\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_cats\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True,\n",
    "    overfit=False,  #  <<<--- DO NOT FORGET TO MAKE IT ``False`` \n",
    "                    #  (``True`` uses only one batch to check pipeline correctness)\n",
    "    callbacks=[\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "        # dl.AveragePrecisionCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"ap\"),\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "        # dl.AUCCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"auc\"),\n",
    "    ],\n",
    "    main_metric=\"iou\", # \"ap/mean\",   \n",
    "    minimize_metric=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On validation set for baseline model **MAP@30** = 0.824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020_hack_baseline_public.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
